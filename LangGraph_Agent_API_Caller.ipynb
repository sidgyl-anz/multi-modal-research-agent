{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidgyl-anz/multi-modal-research-agent/blob/main/LangGraph_Agent_API_Caller.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igepTypHVXOf"
      },
      "source": [
        "# Calling the LangGraph Multi-Modal Research Agent API\n",
        "\n",
        "This notebook demonstrates how to call the deployed LangGraph research agent API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qZejEp7sVXOh"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install requests -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KV-N0qUFVXOi"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6XouZRPVXOj"
      },
      "source": [
        "## Test Case 1: \"Topic Only\" Research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obUOk5prVXOj",
        "outputId": "f77a2aac-0ff3-49a4-a848-ed09e49f1498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Test Case 1: Topic Only Research ---\n",
            "\n",
            "--- Full Response for Topic Only Research ---\n",
            "{\n",
            "  \"report\": \"# Research Synthesis on 'The history and evolution of functional programming languages'\\n\\n**Research Synthesis on 'The history and evolution of functional programming languages'**\\n\\n1.  **Introduction**\\n    This report provides a comprehensive synthesis of the history and evolution of functional programming (FP) languages. Functional programming is a paradigm rooted in mathematical functions, emphasizing immutability and the avoidance of mutable state. As no specific company information was provided in the input materials, this report focuses exclusively on the historical development, key concepts, and significant milestones of functional programming languages. The purpose of this report is to analyze and consolidate the provided information to present a clear and structured overview of this influential programming paradigm.\\n\\n2.  **Key Findings and Thematic Analysis**\\n\\n    The origins of functional programming trace back to the 1930s, predating modern computing. Its conceptual foundation lies in mathematical logic, specifically Alonzo Church's lambda calculus, developed in the 1930s. Lambda calculus, a formal system for computation based on function abstraction and application, was proven Turing-complete in 1937, establishing its theoretical completeness. Church's subsequent work on simply typed lambda calculus laid the groundwork for statically typed functional languages.\\n\\n    The first practical functional programming languages emerged in the mid-20th century. Lisp (List Processing), created by John McCarthy in 1958, is widely recognized as the pioneering functional language. It introduced fundamental FP concepts such as symbolic computation, recursion, and first-class functions, alongside innovations like garbage collection. While Lisp was multi-paradigm, its functional core significantly influenced subsequent developments, particularly in artificial intelligence research. Other early languages included Kenneth E. Iverson's APL in the early 1960s, known for its concise notation, and Peter Landin's ISWIM in the mid-1960s, which explored the connection between ALGOL 60 and lambda calculus. John Backus's FP language in 1977, though not widely adopted, brought renewed academic interest to functional programming through his influential 1978 Turing Award lecture.\\n\\n    The period from the 1970s to the 1990s saw significant advancements and the emergence of more sophisticated functional languages. ML (Meta Language), developed by Robin Milner's group in 1973, introduced the polymorphic Hindley-Milner type system, enabling automatic type inference, along with pattern matching and function currying. ML proved highly influential in programming language research, with Standard ML being formally defined in 1990. Scheme, a minimalist Lisp dialect from 1975, focused on functional purity, incorporating lexical scoping and tail-call optimization for efficient recursion. Miranda (1985) by David Turner was a purely functional language that pioneered lazy evaluation by default and significantly influenced Haskell. Haskell, standardized by a committee in the late 1980s, became a benchmark for purely functional programming, featuring lazy evaluation, type classes, monads, static typing with type inference, and a strong emphasis on mathematical purity (functions without side effects). Concurrently, Erlang, developed at Ericsson from 1986 by Joe Armstrong, Robert Virding, and Mike Williams, was designed for highly concurrent and fault-tolerant telecommunications systems, characterized by lightweight, memory-isolated processes communicating via asynchronous message passing, immutable data, and pattern matching.\\n\\n    In the modern era, functional programming has experienced a notable resurgence and broadened its influence across the software development landscape. Its core principles, such as immutability, pure functions, and higher-order functions, are increasingly integrated into mainstream object-oriented languages. Examples include Java's adoption of lambda functions, C#'s incorporation of pattern matching and immutability, JavaScript's first-class functions and closures, and Python's long-standing support for `lambda`, `map`, `reduce`, and `filter`. This cross-pollination highlights the paradigm's practical benefits. Furthermore, new functional languages continue to emerge and evolve, often leveraging existing platforms. Scala (2003) by Martin Odersky is a hybrid functional-object-oriented language running on the Java Virtual Machine (JVM). Clojure (2007) by Rich Hickey is a Lisp dialect also running on the JVM. Elixir (2011) by Jos\\u00e9 Valim is a dynamic, functional language built on Erlang's BEAM virtual machine, designed for scalable web applications. The enduring appeal of functional programming stems from its ability to produce more predictable, testable, and maintainable code, making it particularly well-suited for concurrent and distributed systems.\\n\\n3.  **Discussion**\\n\\n    The historical trajectory of functional programming reveals a consistent evolution driven by foundational mathematical principles and a continuous pursuit of more robust and reliable software development paradigms. A significant pattern is the gradual shift from theoretical concepts like lambda calculus to practical language implementations, with each generation building upon the innovations of its predecessors, particularly in areas like type systems (e.g., Hindley-Milner in ML, type classes in Haskell) and concurrency models (e.g., Erlang's actor model). The increasing integration of functional concepts into mainstream multi-paradigm languages underscores the paradigm's proven value in addressing modern software challenges, such as managing complexity, concurrency, and state.\\n\\n    While the provided materials offer a detailed chronological account of languages and their features, a limitation is the absence of specific real-world adoption statistics, industry case studies, or detailed comparisons of performance characteristics across different functional languages. The report primarily focuses on the \\\"what\\\" and \\\"when\\\" of language development and conceptual evolution, rather than the \\\"how widely\\\" or \\\"why specifically\\\" certain languages gained commercial traction beyond their initial design goals.\\n\\n4.  **Conclusion**\\n\\n    In summary, functional programming has a rich and extensive history, originating from abstract mathematical logic and evolving through distinct eras of language development. From the foundational lambda calculus and pioneering Lisp to the sophisticated type systems of ML and Haskell, and the concurrent capabilities of Erlang, the paradigm has consistently pushed the boundaries of programming language design. Its modern influence is evident not only in new dedicated functional languages but also in the widespread adoption of its core principles by mainstream languages, reinforcing its value for building predictable, testable, and scalable software. The journey of functional programming is a testament to the enduring power of mathematical rigor in shaping the future of computation.\\n\\n\\n## Additional Research Sources\\n1. adabeat.com\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHKIkQOcc8uGzT761MwTRDxYX5k_9CqszB_pU1LwB_8k8xUZbunkZ0-dge3i2_hAUOPU0fftV2__QSh6CBc1nToETfdDJnPOmqLakUWrujfIJwvfr9nGCE2YmIvSYKuMjnVAlqvYFE3ea70hwJ15AJji1DRUedzQUg=\\n2. medium.com\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH0tWNrhdkgLQBYq22p5Y6CCVQGysr-rbfzItnzHlTl76PGljp5XLLQ5yvngUv-Omg7xYHyWolT-00LoBFQtSWx-OQDBmkzp0K6ZnaQy2_lesEfuPZOlmvZZmnM8Vkzc2iBljbQbsAI07SIDl9kSkyV_KYWt7smn9s9Pwq1LBYJXuJ_st1kv87R-qqxwW5Jo72dK65PwA==\\n3. medium.com\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEhKEdeO6cdj_m_2--CKuWx2fIbvvj5bq1xh3rga3GeHjo4XHhoWpa6JIBi-q8mK8XplN4E3qV_WzKIMj_HHRl_sQ7LeqfVzuSo1UeXzH_b1g0lhbB2FANnCXEvPJ2PjZPNc2ZECQgQajURkmenyJiEFphbu68S0tFcdNnbb4977MUZ7sH2zcrzP9d51wkQ9uKqKIPMYdpi73jk4obH97s3VaJSfl3HSfhUyuPDXdoU3wGg94ewDSyWLlY4GpfipemQ\\n4. wikipedia.org\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF0UYyEOQhGgFGd50Tip9dx36DjrBGtnI1c3kP1hYr421MiugnTrawDLW4lvBqUqxL1TVokUTPKsxWMUj0UOVOU_38r7rg8TyF0MN3zAeLLcSp_4e_jkOIAnmWCyi0VyMx6C8Huwt5zhg==\\n5. psu.edu\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG2ymPyHkqcdMrCD_Yv-2pTLRJpM5AblyEvdnmudC4ZFoJ3lHBOg79_a3YIk7zTIS1pnmBMVATRhzDTNiWH-LhvDTKd_JAqaduwsFjGgc7rFZHOWG3FhCl1-Y7cdZ_d4iu9K7GU4xY6PaX5d37qGEP_yLjd4jEc\\n6. saylor.org\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGOkgeebKFs-1u_IaDi_Se1EGctLetrb9SQukvirI8KCI_m0tuPFWhaVbat98uaE8LbG4jN6aYv5DhVak1yYd27sM9z9c1cKhVC8mlWe0ZtPPlSPP8WYqIogJ7Leuo83NVCw80NEuOTktdh4SRnitOfV_-GWczGU98Etw==\\n7. youtube.com\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEbe62olkfhmlMrWysXY34ZFW8ONNNn5beLNypUYpIlBO6Noz6X23jzUNRhfhGJoXom_aQeQ8xwKJ5hABsvuG6pD8J7p7lryEHNXtAxgF60l9szM0V5zVdmWeH7YtmogFL9JCWQCt8=\\n8. wikipedia.org\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEKXM43ltil8BatKrZxd5w-hZn7bpMyBnmeqmr6lh5hlLQ4l-IxvI5NR2b1NOTazKWOxK9-5DYnCd6b0_P5Nb8PJfCAhkRQsSUiwJx8RPN_2j-JDaiz1rCUZZDR4fIRMOZfUU1ElHB5nQGcjg6wQWM=\\n9. wikipedia.org\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqnZlDOYUoLj1cqkm-bmHkXNDwwNxqiGjOMcJc9sB7pzt-8zkfWpNsBD3edh1NeH7RfKBLGVXrJ4ZqsomplS9CC8rstSOGpd4hnAJlpWTOqdHdT8Jlz_jfkBz1gTrTET2BJ_tW8GxYrPH6wZfE1wPP3xH6KQ==\\n10. mit.edu\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFd2jpsn3Mk_WckRKCps6uUlxQxpd_VyfPk3GDoupLZzXImie5RaCRcSv2vm6KS9gE0dTgTy6sQSPSc92Q0SAr7Mnwa9eud2KpjuaK9bTRO3vrSUQOWJJkBUoGrO-NE9ZHWOsd39xLYF0clOoSuPU8asoyXcuIgT7M4g0T72VUA1YwXSRNStXZXsjwZ\\n11. sigplan.org\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGDDqVcbDJayNym4BhAvBe00-vzLbPlh-n2uS-J1iMpX-prPb0aO1KUZnxltXNyehB4R2TPH-ZYbSyM6ARo1rbHK3b-pnefkzNz4e7lH6Q_s3GAnSYUJO2y9P-pU3GUdrP8qJIXMBXmO7NbcXXZYpDyZWnTg7DD5brwJ5b-vu3yfz_88CILrnM\\n12. wikipedia.org\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHjbKzDi6v2E-fUFFdsoqSRXmQp_9FiR9YL9kgCNQ8-Odi7dy-ZVGR5my2q7sx12zU-86LBwmRSDX66QB3DrtVvPRq-Mz45DqKLk6NFdf4k4rXGAlNfOWBhp97Rw41NDOY15kopNFi0p2mkdopz0YXVxP0=\\n13. washington.edu\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjzJiplCBUbAlH32KkG1e3X-7L2AcbqWAYIPcSYI4WX2RrnUCRaTMUyjVmDMMKN30tGjg1eiXO7ZsYpOKKXGkidJu8PkJcFBnxKaBbZ8vZCgqDboNm9nfuU9eO5YQmPXG3Cv6_GQuDpkbNpHFodDLa6vO7IeIG42YO21IX9JNM60ekJxM5z3tK5lU=\\n14. github.io\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHZfqQwB1jXqn0jN-DIAFcC8fqbIwlt7dtkqqJF067xkUDStq9_J4vr-eJmtxuzr4naQq-g3TAREQx7yORFm2zGoHXFWn-IprcQLTGqjxfOhWpzDRnCQZhjeHU9xXyyrDod5UuZB2G5CPUs8wQQyA==\\n15. mylens.ai\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE1HR6s1TM9No0WIKwtwlUcLDIqEc1MlRpOXhWqBek3FDgzVdtLZQfxy7e0bsoHsDNdPLPxdVtTh8HthiF29TlgLwz8-5H4g6FwHA_sg-7GdOa9XJoDplX5qHTr0cVYIL3RtOhJi1paZTmMCBUJRmtJQ29Zuyd81wgntAIkYbB2pFh2DiSuCRwn2Iu2ernB8dreS1YqyfSW\\n16. medium.com\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqAkRXY-_rk6PwkRVgsEHbpOXEjrT7rPJqzyXquMVNuFCRIB-3E3gOhhT3a4AkB6WBRP4IPQjr-x6_Lm9Dg7SiQVUHTJ0PQcYa7-Sf4MXvlLR0WDF9nREtC3iHBIyuzOU0bgTuqfYN9VuDIH3CunTBi2nmsBjsnYZw8qf0n_G65PMm\\n17. saylor.org\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHtcvbtu-7UM6VpZTBz9PaF8Q1JRLUfiIeMh9NLha1grQNSq2SuVDT5oKT9QGTmBXNiGI_hgLjyADw5TFF44FezE8drzzdt10w6YdmWj7hucVCD42JwDFhjtfSpmu1q7NSQeGOGqUjDUabHsCkF0YtDjFeTOhN1j6W2eZwZ8ww=\\n18. dev.to\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQESaMvE4kOxT4W8DHYDZDQybR1GtFr1WscCW3oZtPgc2SXNZP3fh44NiX9kKL-fmdzSfuve4Z2LBUi3xrIHbBo7bFuiB9OLYr-I4VoRE4676KJLLI9yIW1QeUnqtAQv8w==\\n19. geeksforgeeks.org\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE8kWyVqqLI7AUdbHpIgkUATqEyvVSwEWnd6w2JSYapaeSb-1Pmua6gGMo0LApWduAof3PJFySo5XkpzfvRrxbNTQjJ2IK8ZO7mauIFlYeCbFaf-hV7lXKfeVwKgE9Mr8ispW_NbmBsUt6S_dv9c_wztpzBjW7XjXwRFfYAEfo=\\n20. wikipedia.org\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOso5Z8J1JmrKKBzX2ABr-nqFmeH5FOqrA1rfp_2JRdlcrJajK7qJK08t3PcK7kfZ08D44hYQdNWc_SzQCgszZD01jGw34K_B89y6TvuO5eGBxftEp5Pw8ICV6NnamwHI=\\n21. erlang.org\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGvmheUwS8tYZcowo3Rg2Y4ctGGPSgMmlr8WyxJjNHt1eyltcs08p_cvJWUljzVhZaVsz3V8jSDbg5oCWd73mZjLfJS0qXhtFewFDOOEW2GT7wMBPIKcTk0REIwCNRR6sk6\\n22. wikipedia.org\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGf6Qo5u97S0DEi0Nc41Hi3J4brifKXznqlaU1aD1slHoBTuBZYk1T0OOycJOUKJL_LWNBPfqE4N35_nCblBmcn4ByWq4FrJSd6MiDuN9tqSa5ntVCSVIrgno4rrfmrl0zGHs8LBxZvC_4DVuxQStgu6gOSlQ0B\\n23. lfe.io\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH1x4Q4Ol0AnNvix7PFd86-zSBjVzsJZQFeJ2lNBN38nQ8PXL1iL5C5kvkbQakpG1i0utj0ps7z-3fcZkOaWits-rWuNU4drQ3Gggo_uT35mGiNdFEagv3ebN2sjh_1KnHOSjM0T_o_jkoQ2zgHxI1Y73zAwuH5cfRk_jn7X-M8peoaiWjERCbZ7p6Uz3n4ZaFc_mQAcW2ikw==\\n24. medium.com\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFrBk1ZQgCFYEJT-XLIXWkSru-l3AF4GzpphR9B6kab62hz9KE1ry_qCipFsdi_0c_aIIbW0y4pZmrPTuUhfT6ms7FSCmTgjKYau1EeonTzboRc8lR9R93tmDCg99MA8YuNKlt2ZxGCf0J2S0z5f-Qo-Bo8vvsE87HZFl7B9W558GSKizlnnYjZ9UrClQ1etJUd0NI-yK0-n6vwzE4Ph9ptNbPvj4H1DWDUKJNvNmdBCipUdxDLED4=\\n25. happihacking.com\\n   https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2EeG1nDt6MbDhA-dqPJg6Lf8L0TcHjfuizZ9zywxFN3iRkpJVDiRnKS_2TfdkBOPZoweWNmx00DRzzYKKDkTMu46p9Ln5JIWpMuItFVXcl7b8gU8n6RUHzNK-ZQ6mpolfV3uKswyafsT4z1Ed6q60EqiI\\n\\n\\n---\\n*Report generated using multi-modal AI research.*\"\n",
            "}\n",
            "\n",
            "\n",
            "--- Test Case 2: Topic Company Leads Research ---\n",
            "\n",
            "--- Full Response for Topic Company Leads Research ---\n",
            "{\n",
            "  \"report\": \"# Research Synthesis on 'The role of Vector Databases in Generative AI applications' in Relation to Pinecone\\n\\n## Research Synthesis on 'The role of Vector Databases in Generative AI applications' in Relation to Pinecone\\n\\n### 1. Introduction\\n\\nThe rapid evolution of Artificial Intelligence, particularly in the domain of generative AI, has brought to the forefront the critical role of specialized data infrastructure. Vector databases are emerging as a fundamental component in enabling these advanced AI applications. This report delves into the significance of vector databases in the context of generative AI, focusing specifically on Pinecone, a prominent company at the vanguard of this technological shift. The purpose of this report is to synthesize and analyze the provided input materials to elucidate the symbiotic relationship between vector databases and generative AI, and to detail Pinecone's specific contributions and market position within this landscape.\\n\\nPinecone has established itself as a leading provider of a cloud-based, fully managed vector database, engineered to address key limitations of large language models (LLMs). By facilitating the storage, indexing, and querying of high-dimensional vector embeddings, Pinecone empowers LLMs with external, real-time, and domain-specific knowledge, thereby enhancing their accuracy, relevance, and contextual understanding.\\n\\n### 2. Key Findings and Thematic Analysis\\n\\nThe input materials highlight that vector databases, exemplified by Pinecone, are indispensable for the practical application and advancement of generative AI. They serve as a crucial mechanism for providing LLMs with long-term memory and access to external, up-to-date, and proprietary data. This capability directly mitigates common LLM challenges such as \\\"hallucinations\\\" and significantly improves the factual accuracy and contextual relevance of AI-generated outputs.\\n\\n**Pinecone's Core Contributions and Use Cases:**\\nPinecone's primary business revolves around its specialized vector database, which stores and enables efficient similarity searches of high-dimensional vector embeddings. These embeddings are numerical representations of unstructured data (text, images, audio, video) that capture semantic meaning. The company's offerings are directly aligned with supporting and enhancing generative AI applications across various critical use cases:\\n\\n*   **Retrieval Augmented Generation (RAG):** Pinecone is a cornerstone of RAG architectures, allowing LLMs to retrieve and leverage relevant information from external data sources. This process is vital for providing accurate, contextually rich, and verifiable answers without the need for constant retraining of the base LLM. Pinecone's serverless architecture is specifically optimized for RAG workloads, underscoring its commitment to this key application.\\n*   **Semantic Search:** Moving beyond traditional keyword matching, Pinecone enables semantic search by understanding the intent and contextual meaning of queries through vector embedding comparisons. This capability is crucial for enhanced AI search experiences, especially on large, proprietary datasets.\\n*   **Long-Term Memory for LLMs:** Pinecone provides a scalable and efficient long-term memory solution for LLMs, allowing them to store and query vast amounts of their own data, leading to more consistent and informed interactions over time.\\n*   **Chatbots and AI Agents:** The database is widely utilized for building intelligent, context-aware chatbots and AI agents. Products like Pinecone Assistant simplify the development of such grounded chat and agent-based applications by abstracting complex steps like chunking, embedding, and model orchestration.\\n*   **Broader Applications:** Beyond text-centric generative AI, Pinecone's capabilities extend to recommendation systems, classification and labeling for active learning, computer vision (e.g., object detection), and anomaly detection by efficiently comparing vector representations of diverse data types.\\n\\n**Key Features and Architectural Advantages:**\\nPinecone's architecture is specifically engineered to meet the demanding requirements of generative AI applications. Its features include ultra-low query latency for high-performance search, even with billions of vectors, ensuring responsive AI applications. The database supports real-time data ingestion and updates, allowing AI models to access the freshest information. Filtered vector search enables precise results by combining vector similarity with metadata filters. Scalability is a core strength, designed to handle billions of high-dimensional vectors through horizontal scaling. The recent shift to a serverless architecture (Pinecone Serverless) significantly optimizes costs and performance for AI workloads, making advanced vector search more accessible. Furthermore, Pinecone emphasizes easy integration, offering user-friendly APIs and seamless compatibility with major AI models and frameworks such as LangChain, OpenAI, Google Generative AI, AWS, and Azure. Its namespace design allows for data isolation and multi-tenancy, crucial for enterprise-level deployments.\\n\\n**Market Position and Business Strategy:**\\nPinecone operates at the intersection of the Database, Cloud, and AI/ML industries, establishing itself as a leading player in the rapidly expanding vector database market. It is recognized as an industry leader, particularly benefiting from the surge in generative AI interest since late 2022, and is identified as the most popular and most used vector database by developers. While its overall market share in the broader database category is niche, this reflects the specialized nature of vector databases. Key competitors include DataStax, Weaviate, and Qdrant.\\n\\nPinecone's product strategy is centered on its core database (available in both pod-based and serverless architectures), supplemented by services like Pinecone Inference for embedding and reranking models, and Pinecone Assistant for simplified RAG development. The company actively fosters integrations and partnerships to streamline AI application development. Its usage-based pricing model caters to a wide range of users, from individual developers to large enterprises. Founded in 2019 by Dr. Edo Liberty, Pinecone's mission is to \\\"Make AI knowledgeable\\\" by providing essential storage and retrieval infrastructure. The company's organizational focus is heavily geared towards product innovation, performance optimization, and ensuring accessibility and ease of use for engineering teams. Its rapid growth, evidenced by thousands of daily sign-ups, indicates a strong product-led growth strategy and a significant emphasis on developer advocacy and self-service. Pinecone has secured substantial funding, totaling $138 million, underscoring investor confidence in its market position and potential.\\n\\n### 3. Discussion\\n\\nThe synthesized information unequivocally demonstrates the pivotal role of vector databases in unlocking the full potential of generative AI applications. The consistent theme across the materials is how these specialized databases address the inherent limitations of LLMs, particularly concerning factual accuracy, real-time knowledge, and contextual understanding. Pinecone stands out as a critical enabler in this ecosystem, not merely as a database provider, but as a comprehensive platform supporting the entire lifecycle of knowledgeable AI applications. Its strategic focus on Retrieval Augmented Generation (RAG) and the development of tools like Pinecone Assistant highlight a deep understanding of the practical challenges faced by developers building production-grade AI systems.\\n\\nThe recurring emphasis on Pinecone's high-performance search, real-time updates, scalability, and serverless architecture underscores the technical sophistication required to meet the demands of dynamic AI workloads. The company's strong market position, evidenced by its popularity among developers and significant funding, reflects the industry's recognition of vector databases as a foundational layer for the next generation of AI. While the provided materials comprehensively detail Pinecone's strengths and applications, they do not explicitly outline any limitations or gaps in the technology or the company's strategy, focusing instead on its innovative solutions and market leadership.\\n\\n### 4. Conclusion\\n\\nIn conclusion, vector databases are indispensable for the effective and accurate deployment of generative AI applications, fundamentally enhancing the capabilities of large language models. Pinecone, through its specialized cloud-based vector database, plays a leading role in this domain by providing essential infrastructure for RAG, semantic search, and long-term memory for LLMs. Its focus on performance, scalability, real-time capabilities, and developer-friendly integrations positions it as a critical enabler for building intelligent, context-aware AI systems. Pinecone's strong market adoption and strategic product development underscore its significant contribution to making AI applications more knowledgeable, reliable, and impactful.\\n\\n\\n---\\n*Report generated using multi-modal AI research.*\",\n",
            "  \"identified_leads\": [],\n",
            "  \"linkedin_cse_contacts\": [\n",
            "    {\n",
            "      \"title\": \"Jenna Pederson - Pinecone | LinkedIn\",\n",
            "      \"link\": \"https://www.linkedin.com/in/jennapederson\",\n",
            "      \"snippet\": \"Jenna is a Staff Developer Advocate at Pinecone with two decades of software engineering\\u2026 \\u00b7 Experience: Pinecone \\u00b7 Location: Greater Minneapolis-St. Paul\\u00a0...\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Arjun Patel - Pinecone | LinkedIn\",\n",
            "      \"link\": \"https://www.linkedin.com/in/arjunkirtipatel\",\n",
            "      \"snippet\": \"Hi! I'm Arjun, a developer advocate with a flair for LLMs and an entrepreneurial spirit\\u2026 \\u00b7 Experience: Pinecone \\u00b7 Education: The University of Chicago\\u00a0...\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Liam Morrison - Pinecone | LinkedIn\",\n",
            "      \"link\": \"https://www.linkedin.com/in/liammorrison\",\n",
            "      \"snippet\": \"Pinecone Graphic \\u00b7 Pinecone. -. New York City Metropolitan Area. -. New York, New ... AWS Certified Solutions Architect - Professional Graphic. AWS Certified\\u00a0...\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Vamshi Enabothala - Pinecone | LinkedIn\",\n",
            "      \"link\": \"https://www.linkedin.com/in/vkenabothala\",\n",
            "      \"snippet\": \"Pinecone Graphic. Pinecone. United States. -. Herndon, Virginia, United ... AWS Certified Solutions Architect - Associate \\u00b7 Amazon Web Services. Issued Feb\\u00a0...\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Zack P. - WorkOS | LinkedIn\",\n",
            "      \"link\": \"https://www.linkedin.com/in/zackproser\",\n",
            "      \"snippet\": \"AWS Solutions Architect Associate Graphic. AWS Solutions Architect Associate ... Pinecone team with on-demand, interactive demos to showcase the power\\u00a0...\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Saim Safdar - AI Solutions Architect / n8n Expert / Data Engineer ...\",\n",
            "      \"link\": \"https://www.linkedin.com/in/saimsafdar/\",\n",
            "      \"snippet\": \"AI Solutions Architect | Data Engineering & Automation Expert | Architecting ... Pinecone, and integrate voice AI assistants for customer support and\\u00a0...\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Payal Singh - Cohere | LinkedIn\",\n",
            "      \"link\": \"https://www.linkedin.com/in/payal-singh-b8bbb64\",\n",
            "      \"snippet\": \"Liam Morrison. 2d. Had a blast presenting at the AWS/Cohere/Pinecone workshop \\\"Building\\u2026 .. ... AWS Certified Solutions Architect \\u2013 Professional \\u00b7 Amazon Web\\u00a0...\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Lana Zhang - Amazon Web Services (AWS) | LinkedIn\",\n",
            "      \"link\": \"https://ca.linkedin.com/in/lanazhang\",\n",
            "      \"snippet\": \"Solutions Architect with an engineering background on the global AWS team\\u2026 ... Had a blast presenting at the AWS/Cohere/Pinecone workshop \\\"Building\\u2026 ..more.\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Amarjeet Singh Khalsa - Senior Solutions Architect - Gathr | LinkedIn\",\n",
            "      \"link\": \"https://www.linkedin.com/in/amarjeet-singh-khalsa-89ab1134\",\n",
            "      \"snippet\": \"Senior Solutions Architect at Gathr Data Inc. \\u00b7 Experience: Gathr \\u00b7 Location: Indore \\u00b7 171 connections on LinkedIn. View Amarjeet Singh Khalsa's profile on\\u00a0...\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Arinze Nsude - Solutions Architect - Awira LLC (Awira.co) | LinkedIn\",\n",
            "      \"link\": \"https://ng.linkedin.com/in/arinze-nsude\",\n",
            "      \"snippet\": \"Solutions Architect. Awira LLC (Awira.co). Jan 2020 - Present 5 years 6 months ... Learn LangChain, Pinecone, OpenAI and Google&#39;s Gemini Models Graphic\\u00a0...\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "BASE_URL = \"https://multi-modal-researcher-675059836631.us-central1.run.app\"\n",
        "GRAPH_ID = \"research_agent\"\n",
        "HEADERS = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "\n",
        "def get_assistant_id_by_graph(graph_id):\n",
        "    response = requests.post(\n",
        "        f\"{BASE_URL}/assistants/search\",\n",
        "        headers=HEADERS,\n",
        "        json={\"graph_id\": graph_id, \"limit\": 1}\n",
        "    )\n",
        "    if response.status_code == 200:\n",
        "        assistants = response.json()\n",
        "        if assistants:\n",
        "            return assistants[0].get(\"assistant_id\")\n",
        "    print(\"Failed to find assistant:\", response.text)\n",
        "    return None\n",
        "\n",
        "\n",
        "def call_research_agent_with_assistant_api(assistant_id, input_payload):\n",
        "    # Step 1: Create a Thread\n",
        "    thread_response = requests.post(\n",
        "        f\"{BASE_URL}/threads\",\n",
        "        headers=HEADERS,\n",
        "        json={\"metadata\": {\"purpose\": \"research_test\"}}\n",
        "    )\n",
        "    if thread_response.status_code != 200:\n",
        "        print(\"Failed to create thread:\", thread_response.text)\n",
        "        return None\n",
        "\n",
        "    thread_id = thread_response.json().get(\"thread_id\")\n",
        "\n",
        "    # Step 2: Run Assistant on Thread (wait for output)\n",
        "    run_response = requests.post(\n",
        "        f\"{BASE_URL}/threads/{thread_id}/runs/wait\",\n",
        "        headers=HEADERS,\n",
        "        json={\n",
        "            \"assistant_id\": assistant_id,\n",
        "            \"input\": input_payload[\"input\"]\n",
        "        }\n",
        "    )\n",
        "    if run_response.status_code != 200:\n",
        "        print(\"Failed to run assistant:\", run_response.text)\n",
        "        return None\n",
        "\n",
        "    return run_response.json()\n",
        "\n",
        "\n",
        "# ---------- Workflow ----------\n",
        "assistant_id = get_assistant_id_by_graph(GRAPH_ID)\n",
        "if not assistant_id:\n",
        "    raise RuntimeError(\"Assistant not found for graph_id 'research_agent'\")\n",
        "\n",
        "print(\"\\n--- Test Case 1: Topic Only Research ---\")\n",
        "topic_only_input_payload = {\n",
        "    \"input\": {\n",
        "        \"topic\": \"The history and evolution of functional programming languages\",\n",
        "        \"research_approach\": \"Topic Only\",\n",
        "        \"company_name\": None,\n",
        "        \"title_areas\": None,\n",
        "        \"video_url\": None,\n",
        "        \"create_podcast\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "results_topic_only = call_research_agent_with_assistant_api(assistant_id, topic_only_input_payload)\n",
        "\n",
        "if results_topic_only:\n",
        "    print(\"\\n--- Full Response for Topic Only Research ---\")\n",
        "    print(json.dumps(results_topic_only, indent=2))\n",
        "else:\n",
        "    print(\"Failed to get results for Topic Only research.\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n--- Test Case 2: Topic Company Leads Research ---\")\n",
        "topic_company_leads_input_payload = {\n",
        "    \"input\": {\n",
        "        \"topic\": \"The role of Vector Databases in Generative AI applications\",\n",
        "        \"research_approach\": \"Topic Company Leads\",\n",
        "        \"company_name\": \"Pinecone\",\n",
        "        \"title_areas\": [\"Solutions Architect\", \"Developer Advocate\", \"Head of Engineering\"],\n",
        "        \"video_url\": None,\n",
        "        \"create_podcast\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "results_company_leads = call_research_agent_with_assistant_api(assistant_id, topic_company_leads_input_payload)\n",
        "\n",
        "if results_company_leads:\n",
        "    print(\"\\n--- Full Response for Topic Company Leads Research ---\")\n",
        "    print(json.dumps(results_company_leads, indent=2))\n",
        "else:\n",
        "    print(\"Failed to get results for Topic Company Leads research.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install requests -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**IMPORTANT:** The `LANGGRAPH_API_INVOKE_URL` below is set to the public URL you provided. If the application is not deployed there or the invoke path is different (e.g., `/research_agent/invoke` instead of just `/invoke`), you'll need to adjust it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration \n",
    "LANGGRAPH_API_INVOKE_URL = \"https://multi-modal-researcher-675059836631.us-central1.run.app/invoke\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function to Call the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_research_agent(payload):\n",
    "    \"\"\"Sends a request to the LangGraph agent and returns the response.\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    print(f\"Sending request to: {LANGGRAPH_API_INVOKE_URL}\")\n",
    "    print(f\"Payload:\\n{json.dumps(payload, indent=2)}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(LANGGRAPH_API_INVOKE_URL, headers=headers, json=payload, timeout=600) # 10 minute timeout\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors (4xx or 5xx)\n",
    "        \n",
    "        print(\"\\n--- Full API Response (raw) ---\")\n",
    "        response_json = response.json()\n",
    "        print(json.dumps(response_json, indent=2))\n",
    "        \n",
    "        # The LangGraph server typically nests the actual graph output under an 'output' key\n",
    "        if isinstance(response_json, dict) and \"output\" in response_json:\n",
    "            return response_json[\"output\"]\n",
    "        else:\n",
    "            print(\"Warning: 'output' key not found in response JSON or response is not a dict, returning full JSON as is.\")\n",
    "            return response_json\n",
    "            \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"\\nHTTP error occurred: {http_err}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        print(f\"\\nConnection error occurred: {conn_err}\")\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        print(f\"\\nTimeout error occurred: {timeout_err}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"\\nAn unexpected request error occurred: {req_err}\")\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        print(f\"\\nFailed to decode JSON response: {json_err}\")\n",
    "        print(f\"Response content probably not JSON: {response.text}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 1: \"Topic Only\" Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Test Case 1: Topic Only Research ---\")\n",
    "topic_only_input_payload = {\n",
    "    \"input\": {\n",
    "        \"topic\": \"The history and evolution of functional programming languages\",\n",
    "        \"research_approach\": \"Topic Only\",\n",
    "        \"company_name\": None,\n",
    "        \"title_areas\": None,\n",
    "        \"video_url\": None, \n",
    "        \"create_podcast\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "results_topic_only = call_research_agent(topic_only_input_payload)\n",
    "\n",
    "if results_topic_only:\n",
    "    print(\"\\n--- Results for Topic Only Research ---\")\n",
    "    print(f\"Report Snippet: {str(results_topic_only.get('report', 'N/A'))[:500]}...\")\n",
    "    if results_topic_only.get('podcast_url'):\n",
    "        print(f\"Podcast URL: {results_topic_only.get('podcast_url')}\")\n",
    "else:\n",
    "    print(\"Failed to get results for Topic Only research.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 2: \"Topic Company Leads\" Research\n",
    "\n",
    "**Note:** For this to fully work, the deployed LangGraph agent's environment must have the necessary API keys set up:\n",
    "- `GEMINI_API_KEY` (for Gemini calls)\n",
    "- `GOOGLE_API_KEY_FOR_CSE` (for CSE LinkedIn search)\n",
    "- `GOOGLE_CSE_ID` (for CSE LinkedIn search)\n",
    "- `GCS_BUCKET_NAME` and `GOOGLE_APPLICATION_CREDENTIALS` (if podcast/report GCS upload is expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n--- Test Case 2: Topic Company Leads Research ---\")\n",
    "topic_company_leads_input_payload = {\n",
    "    \"input\": {\n",
    "        \"topic\": \"The role of Vector Databases in Generative AI applications\",\n",
    "        \"research_approach\": \"Topic Company Leads\",\n",
    "        \"company_name\": \"Pinecone\",\n",
    "        \"title_areas\": [\"Solutions Architect\", \"Developer Advocate\", \"Head of Engineering\"],\n",
    "        \"video_url\": None,\n",
    "        \"create_podcast\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "results_company_leads = call_research_agent(topic_company_leads_input_payload)\n",
    "\n",
    "if results_company_leads:\n",
    "    print(\"\\n--- Results for Topic Company Leads Research ---\")\n",
    "    print(f\"Report Snippet: {str(results_company_leads.get('report', 'N/A'))[:500]}...\")\n",
    "    \n",
    print(\"\nIdentified B2B Opportunities (from Gemini):\")\n",
    "    b2b_opportunities = results_company_leads.get('identified_leads') # This field now contains B2B Opportunities\n",
    "    if b2b_opportunities:\n",
    "        print(f\"  Found {len(b2b_opportunities)} B2B Opportunity object(s). Showing details for the first few:\")\n",
    "        for i, opportunity in enumerate(b2b_opportunities[:2]): # Print first 2 opportunities\n",
    "            print(f\"\\n  Opportunity {i+1}:\")\n",
    "            print(f\"    Name: {opportunity.get('opportunity_name', 'N/A')}\")\n",
    "            print(f\"    Description: {opportunity.get('opportunity_description', 'N/A')}\")\n",
    "            print(f\"    Relevant Departments: {', '.join(opportunity.get('relevant_departments', [])) if opportunity.get('relevant_departments') else 'N/A'}\")\n",
    "            \n",
    "            contact_points = opportunity.get('contact_points', [])\n",
    "            if contact_points:\n",
    "                print(\"    Contact Points for this Opportunity:\")\n",
    "                for cp_idx, cp in enumerate(contact_points[:3]): # Show first 3 contact points\n",
    "                    print(f\"      - Contact {cp_idx+1}: {cp.get('contact_name', 'N/A')} ({cp.get('contact_title', 'N/A')})\")\n",
    "                    print(f\"        Dept: {cp.get('contact_department', 'N/A')}, LinkedIn: {cp.get('contact_linkedin_url', 'N/A')}, Relevance: {cp.get('contact_relevance_to_opportunity', 'N/A')}\")\n",
    "            else:\n",
    "                print(\"    No specific contact points identified for this opportunity.\")\n",
    "\n",
    "            decision_makers = opportunity.get('potential_decision_makers_for_opportunity', [])\n",
    "            if decision_makers:\n",
    "                print(\"    Potential Decision-Makers for this Opportunity Type:\")\n",
    "                for dm_idx, dm in enumerate(decision_makers[:2]): # Show first 2 DMs\n",
    "                    print(f\"      - DM {dm_idx+1}: {dm.get('dm_name', 'N/A')} ({dm.get('dm_title', 'N/A')}) - Rationale: {dm.get('dm_rationale', 'N/A')}\")\n",
    "            else:\n",
    "                print(\"    No potential decision-makers identified for this opportunity type.\")\n",
    "    else:\n",
    "        print(\"  No B2B opportunities identified by Gemini or field not present.\")\n",
    "        \n",
    "    print(\"\\nLinkedIn Contacts (from CSE):\")\n",
    "    cse_contacts = results_company_leads.get('linkedin_cse_contacts')\n",
    "    if cse_contacts:\n",
    "        for i, contact in enumerate(cse_contacts[:3]): # Print first 3 CSE contacts\n",
    "            print(f\"  Contact {i+1}:\")\n",
    "            print(f\"    Title: {contact.get('title')}\")\n",
    "            print(f\"    Link: {contact.get('link')}\")\n",
    "            print(f\"    Snippet: {contact.get('snippet')}\")\n",
    "    else:\n",
    "        print(\"  No LinkedIn contacts found via CSE or field not present.\")\n",
    "\n",
    "    if results_company_leads.get('podcast_url'):\n",
    "        print(f\"\\nPodcast URL: {results_company_leads.get('podcast_url')}\")\n",
    "else:\n",
    "    print(\"Failed to get results for Topic Company Leads research.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a template for interacting with the LangGraph research agent. Remember to:\n",
    "1. Ensure your LangGraph agent is deployed and accessible at the `LANGGRAPH_API_INVOKE_URL` specified.\n",
    "2. Verify the invoke path (e.g., `/invoke` or `/<graph_id>/invoke`).\n",
    "3. Ensure all necessary API keys (`GEMINI_API_KEY`, `GOOGLE_API_KEY_FOR_CSE`, `GOOGLE_CSE_ID`) are correctly configured in the environment where your LangGraph agent is running for full functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
=======
  "nbformat": 4,
  "nbformat_minor": 0
}
