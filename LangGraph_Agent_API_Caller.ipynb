{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidgyl-anz/multi-modal-research-agent/blob/main/LangGraph_Agent_API_Caller.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igepTypHVXOf"
      },
      "source": [
        "# Calling the LangGraph Multi-Modal Research Agent API\n",
        "\n",
        "This notebook demonstrates how to call the deployed LangGraph research agent API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qZejEp7sVXOh"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install requests -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV-N0qUFVXOi"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6XouZRPVXOj"
      },
      "source": [
        "## Test Case 1: \"Topic Only\" Research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obUOk5prVXOj",
        "outputId": "b372eed1-567b-4c55-fbde-2582d3920a1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Test Case 1: Topic Only Research ---\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "BASE_URL = \"https://multi-modal-researcher-675059836631.us-central1.run.app\"\n",
        "GRAPH_ID = \"research_agent\"\n",
        "HEADERS = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "\n",
        "def get_assistant_id_by_graph(graph_id):\n",
        "    response = requests.post(\n",
        "        f\"{BASE_URL}/assistants/search\",\n",
        "        headers=HEADERS,\n",
        "        json={\"graph_id\": graph_id, \"limit\": 1}\n",
        "    )\n",
        "    if response.status_code == 200:\n",
        "        assistants = response.json()\n",
        "        if assistants:\n",
        "            return assistants[0].get(\"assistant_id\")\n",
        "    print(\"Failed to find assistant:\", response.text)\n",
        "    return None\n",
        "\n",
        "\n",
        "def call_research_agent_with_assistant_api(assistant_id, input_payload):\n",
        "    # Step 1: Create a Thread\n",
        "    thread_response = requests.post(\n",
        "        f\"{BASE_URL}/threads\",\n",
        "        headers=HEADERS,\n",
        "        json={\"metadata\": {\"purpose\": \"research_test\"}}\n",
        "    )\n",
        "    if thread_response.status_code != 200:\n",
        "        print(\"Failed to create thread:\", thread_response.text)\n",
        "        return None\n",
        "\n",
        "    thread_id = thread_response.json().get(\"thread_id\")\n",
        "\n",
        "    # Step 2: Run Assistant on Thread (wait for output)\n",
        "    run_response = requests.post(\n",
        "        f\"{BASE_URL}/threads/{thread_id}/runs/wait\",\n",
        "        headers=HEADERS,\n",
        "        json={\n",
        "            \"assistant_id\": assistant_id,\n",
        "            \"input\": input_payload[\"input\"]\n",
        "        }\n",
        "    )\n",
        "    if run_response.status_code != 200:\n",
        "        print(\"Failed to run assistant:\", run_response.text)\n",
        "        return None\n",
        "\n",
        "    return run_response.json()\n",
        "\n",
        "\n",
        "# ---------- Workflow ----------\n",
        "assistant_id = get_assistant_id_by_graph(GRAPH_ID)\n",
        "if not assistant_id:\n",
        "    raise RuntimeError(\"Assistant not found for graph_id 'research_agent'\")\n",
        "\n",
        "print(\"\\n--- Test Case 1: Topic Only Research ---\")\n",
        "topic_only_input_payload = {\n",
        "    \"input\": {\n",
        "        \"topic\": \"The history and evolution of functional programming languages\",\n",
        "        \"research_approach\": \"Topic Only\",\n",
        "        \"company_name\": None,\n",
        "        \"title_areas\": None,\n",
        "        \"video_url\": None,\n",
        "        \"create_podcast\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "results_topic_only = call_research_agent_with_assistant_api(assistant_id, topic_only_input_payload)\n",
        "\n",
        "if results_topic_only:\n",
        "    print(\"\\n--- Full Response for Topic Only Research ---\")\n",
        "    print(json.dumps(results_topic_only, indent=2))\n",
        "else:\n",
        "    print(\"Failed to get results for Topic Only research.\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n--- Test Case 2: Topic Company Leads Research ---\")\n",
        "topic_company_leads_input_payload = {\n",
        "    \"input\": {\n",
        "        \"topic\": \"The role of Vector Databases in Generative AI applications\",\n",
        "        \"research_approach\": \"Topic Company Leads\",\n",
        "        \"company_name\": \"Pinecone\",\n",
        "        \"title_areas\": [\"Solutions Architect\", \"Developer Advocate\", \"Head of Engineering\"],\n",
        "        \"video_url\": None,\n",
        "        \"create_podcast\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "results_company_leads = call_research_agent_with_assistant_api(assistant_id, topic_company_leads_input_payload)\n",
        "\n",
        "if results_company_leads:\n",
        "    print(\"\\n--- Full Response for Topic Company Leads Research ---\")\n",
        "    print(json.dumps(results_company_leads, indent=2))\n",
        "else:\n",
        "    print(\"Failed to get results for Topic Company Leads research.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}